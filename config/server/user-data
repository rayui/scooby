#cloud-config
# vim: syntax=yaml
#

# Set your hostname here, the manage_etc_hosts will update the hosts file entries as well
hostname: buffy 
manage_etc_hosts: false 

package_update: false
package_upgrade: false
package_reboot_if_required: true

packages:
  - vim
  - dnsmasq
  - bind9-utils
  - iptables
  - iptables-persistent
  - libarchive-tools
  - nfs-server
  - lighttpd
  - git

users:
  - default

write_files:
- path: /etc/hosts
  permissions: 0644
  owner: 'root:root'
  content: |
    127.0.1.1 buffy buffy
    127.0.0.1 localhost
    ::1 ip6-localhost ip6-loopback
    fe00::0 ip6-localnet
    ff00::0 ip6-mcastprefix
    ff02::1 ip6-allnodes
    ff02::2 ip6-allrouters
    ff02::3 ip6-allhosts

    192.168.1.64 prometheus.buffy
    192.168.1.64 grafana.buffy
    192.168.64.1 buffy
    192.168.64.1 buffy.scooby
    192.168.64.1 buffy.scooby.eastwood
    192.168.64.64 willow
    192.168.64.64 willow.scooby
    192.168.64.64 willow.scooby.eastwood

- path: /etc/dnsmasq.conf
  permissions: 0644
  owner: 'root:root'
  content: |
    listen-address=::1,127.0.0.1,192.168.64.1
    port=53
    domain=scooby.eastwood
    interface=eth1
    bind-interfaces
    dhcp-authoritative
    dhcp-range=interface:eth1,192.168.64.64,192.168.64.191
    dhcp-option=66,192.168.64.1
    dhcp-option=option:router,192.168.64.1
    server=192.168.1.2
    server=1.1.1.1
    expand-hosts
    log-queries
    log-dhcp
    enable-tftp
    tftp-root=/tftpboot/
    pxe-service=0,"Raspberry Pi Boot"
    #dhcp-ignore=tag:!known
    dhcp-host=net:willow,b8:27:eb:86:53:3c,willow,192.168.64.64,24h
    dhcp-option=net:willow,17,192.168.64.1:/nfs/willow

- path: /etc/iptables/rules.v4
  permissions: 0644
  owner: 'root:root'
  content: |
    *filter
    :INPUT ACCEPT [5610749:3014554876]
    :FORWARD ACCEPT [0:0]
    :OUTPUT ACCEPT [4835418:1436051403]
    -A FORWARD -i eth0 -o eth1 -m state --state RELATED,ESTABLISHED -j ACCEPT
    -A FORWARD -i eth1 -o eth0 -j ACCEPT
    COMMIT
    *nat
    :PREROUTING ACCEPT [95416:7110036]
    :INPUT ACCEPT [94707:7084699]
    :OUTPUT ACCEPT [96642:5810991]
    :POSTROUTING ACCEPT [96483:5789063]
    -A POSTROUTING -o eth0 -j MASQUERADE
    COMMIT

- path: /etc/exports
  permissions: 0644
  owner: 'root:root'
  content: |
    /nfs/willow		willow(rw,sync,no_subtree_check,no_root_squash)

- path: /etc/lighttpd/lighttpd.conf
  permissions: 0644
  owner: 'root:root'
  content: |
    server.bind 		= "192.168.64.1"
    server.port 		= 8080
    server.document-root        = "/var/www/html"
    server.upload-dirs          = ( "/var/cache/lighttpd/uploads" )
    server.errorlog             = "/var/log/lighttpd/error.log"
    server.pid-file             = "/run/lighttpd.pid"
    server.username             = "www-data"
    server.groupname            = "www-data"

- path: /etc/oem/kube-config.yaml
  permissions: 0600
  owner: 'root:root'
  content: |
    kube-controller-manager-arg:
    - "bind-address=0.0.0.0"
    kube-proxy-arg:
    - "metrics-bind-address=0.0.0.0"
    kube-scheduler-arg:
    - "bind-address=0.0.0.0"
    # Controller Manager exposes etcd sqllite metrics
    etcd-expose-metrics: true

- path: /etc/oem/prometheus-stack-values.yaml
  permissions: 0600
  owner: 'root:root'
  content: |
    # k3s uses sqllite, so we cannot monitor etcd in the same way
    defaultRules:
      rules:
        etcd: false

    kubeEtcd:
      enabled: false

    # matched to service port 'prom-stack-kube-prometheus-kube-controller-manager' -n kube-system
    kubeControllerManager:
      enabled: true
      endpoints: ['192.168.64.1']
      service:
        enabled: true
        port: 10252
        targetPort: 10252
      serviceMonitor:
        enabled: true
        https: false

    # matched to service port 'prom-stack-kube-prometheus-kube-scheduler' -n kube-system
    kubeScheduler:
      enabled: true
      endpoints: ['192.168.64.1']
      service:
        enabled: true
        port: 10251
        targetPort: 10251
      serviceMonitor:
        enabled: true
        https: false

    # matched to service port 'prom-stack-kube-prometheus-kube-proxy' -n kube-system
    kubeProxy:
      enabled: true
      endpoints: ['192.168.64.1']
      service:
        enabled: true
        port: 10249
        targetPort: 10249


    alertmanager:
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: nginx
        hosts: ['alertmanager.buffy']
        paths: ['/']
        tls:
        - secretName: tls-credential
          hosts:
          - alertmanager.buffy

      alertmanagerSpec:
        storage:
          volumeClaimTemplate:
            spec:
              storageClassName: local-path
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 5Gi
        externalUrl: https://alertmanager.buffy/
        routePrefix: /

      tplConfig: false
      # INSTEAD rely on helm --set-file "alertmanager.templateFiles.email\.default\.html\.tmpl"=pathtofile
      #templateFiles:
      #  email.default.html.tmpl: |-
      #    ... 
      #  email.default.txt.tmpl: |-
      #    ...

      config:
        global:
          resolve_timeout: 5m
          # global smtp settings
          smtp_from: amgr@buffy
          smtp_smarthost: 10.43.235.116:1025
          smtp_require_tls: false

        route:
          group_by: ['alertname']
          group_wait: 2s # not default 30
          group_interval: 30s # not default 5m
          repeat_interval: 4h # not default 12h
          receiver: email_platform
          routes:
          - receiver: 'null'
            matchers:
              - alertname =~ "InfoInhibitor|Watchdog"
          - receiver: email_platform
            continue: true
        receivers:
        - name: email_platform
          email_configs:
          - to: platform@buffy
            send_resolved: true
            headers:
              subject: "{{ .Status | toUpper }} {{ .CommonLabels.env }}:{{ .CommonLabels.cluster }} {{ .CommonLabels.alertname }}"
            #html: "{{ range .Alerts }}{{ .Annotations.description }}<br/>{{ end }}"
            # proper syntax for external template ready by alertmanager
            # defining both these values will send email in multipart/alternative
            #html: '{{ template "emaildefaulthtml" . }}'
            #text: '{{ template "emaildefaulttxt" . }}'

        - name: 'null'
        templates:
        - '/etc/alertmanager/config/*.tmpl'

    grafana:
      # username is 'admin'
      adminPassword: prom-operator
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: nginx
        hosts: ['grafana.buffy']
        path: "/"
        tls:
        - secretName: tls-credential
          hosts:
          - grafana.buffy

    prometheus:
      ingress:
        enabled: true
        annotations:
          kubernetes.io/ingress.class: nginx
        hosts: ['prometheus.buffy']
        paths: ['/']
        tls:
        - secretName: tls-credential
          hosts:
          - prometheus.buffy

      prometheusSpec:
        externalUrl: "https://prometheus.buffy/"
        routePrefix: /

        # do not require new PrometheusRule to have all the helm labels in order to match
        ruleSelectorNilUsesHelmValues: false

        # additional scrape job
        additionalScrapeConfigs:
          - job_name: kubernetes-pod-endpoints
            kubernetes_sd_configs:
            - {role: pod}
            relabel_configs:
            - action: keep
              regex: true
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
            - action: keep
              regex: true
              source_labels: [__meta_kubernetes_pod_annotationpresent_prometheus_io_port]
            - action: drop
              regex: (kube-system|prom)
              source_labels: [__meta_kubernetes_namespace]
            - action: replace
              regex: (https?)
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
              target_label: __metrics_path__
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
              target_label: __address__
            - {action: labelmap, regex: __meta_kubernetes_pod_label_(.+)}
            - action: replace
              source_labels: [__meta_kubernetes_namespace]
              target_label: kubernetes_namespace
            - action: replace
              source_labels: [__meta_kubernetes_service_name]
              target_label: kubernetes_name
          - job_name: kubernetes-service-endpoints
            kubernetes_sd_configs:
            - {role: service}
            relabel_configs:
            - action: keep
              regex: true
              source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
            - action: drop
              regex: (kube-system|prom)
              source_labels: [__meta_kubernetes_namespace]
            - action: keep
              regex: .*metrics
              source_labels: [__meta_kubernetes_service_port_name]
            - action: replace
              regex: (https?)
              source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
              target_label: __scheme__
            - action: replace
              regex: (.+)
              source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
              target_label: __metrics_path__
            - action: replace
              regex: ([^:]+)(?::\d+)?;(\d+)
              replacement: $1:$2
              source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
              target_label: __address__
            - {action: labelmap, regex: __meta_kubernetes_service_label_(.+)}
            - action: replace
              source_labels: [__meta_kubernetes_namespace]
              target_label: kubernetes_namespace
            - action: replace
              source_labels: [__meta_kubernetes_service_name]
              target_label: kubernetes_name


        # external labels will be common for all alerts and available for templating in AlertManager
        externalLabels: {'cluster': 'k3s', 'env': 'dev', 'jumpbox': 'localhost.local'}

        storageSpec:
          volumeClaimTemplate:
            spec:
              storageClassName: local-path
              accessModes: ["ReadWriteOnce"]
              resources:
                requests:
                  storage: 5Gi


bootcmd:
  #this must be here for dnsmasq startup
  - mkdir -p /tftpboot
  #this must be here for nfs-kernel-server startup
  - mkdir -p /nfs/willow
  - sed -i '$s/$/ cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory/' /boot/cmdline.txt

runcmd:
#CREATE CONFIG DIR
  - mkdir -p /home/spike/.config
  - chown -R spike:spike /home/spike/.config/

#SSH KEYS
  - ssh-keygen -q -f /home/spike/.ssh/id_ed25519 -N "" -t ed25519
  - cat /home/spike/.ssh/id_ed25519.pub >> /home/spike/.ssh/authorized_keys
  - chown -R spike:spike /home/spike/.ssh/

#SETUP TFTPBOOT
  - cp /boot/bootcode.bin /tftpboot

#WILLOW ROOTFS
  - mkdir -p /mnt/rootfs
  - mount -o loop,offset=272629760 /images/rpi-cloud-init-raspios-bullseye-arm64.img /mnt/rootfs
  - rsync -xa --progress /mnt/rootfs/* /nfs/willow
  - cp /agents/willow/fstab /nfs/willow/etc
  - umount /mnt/rootfs
  - sleep 5

#WILLOW BOOT
  - mkdir -p /mnt/boot
  - mount -o loop,offset=4194304 /images/rpi-cloud-init-raspios-bullseye-arm64.img /mnt/boot
  - rsync -xa --progress /mnt/boot/* /nfs/willow/boot
  - umount /mnt/boot
  - sleep 5
  - cp /agents/willow/user-data /nfs/willow/boot
  - cp /agents/willow/meta-data /nfs/willow/boot 
  - cp /agents/willow/network-config /nfs/willow/boot 
  - cp /agents/willow/cmdline.txt /nfs/willow/boot 
  - cp /agents/willow/ssh /nfs/willow/boot
  - ln -s /nfs/willow/boot /tftpboot/9686533c

#WILLOW CLOUD CONFIG 
  - mkdir -p /var/www/html/cloud-config/willow
  - ln -s /nfs/willow/boot/user-data /var/www/html/cloud-config/willow
  - ln -s /nfs/willow/boot/meta-data /var/www/html/cloud-config/willow

#WILLOW USER
  - cp /home/spike/.ssh/id_ed25519 /nfs/willow/etc/ssh/spike_ed25519_key
  - cp /home/spike/.ssh/id_ed25519.pub /nfs/willow/etc/ssh/spike_ed25519_key.pub

#K3S
  # local mount point for rancher see here: https://github.com/containerd/containerd/issues/5464
  - mkdir -p /nfs/willow/var/lib/rancher
  - cd /tmp && curl -sLS https://get.k3sup.dev | sh
  - cp /etc/oem/kube-config.yaml /home/spike/.config/
  # https://github.com/k3s-io/k3s/issues/535#issuecomment-863188327 
  - k3sup install --ip 192.168.1.64 --user spike --ssh-key "/home/spike/.ssh/id_ed25519" --k3s-extra-args "--flannel-backend=host-gw --config /home/spike/.config/kube-config.yaml --disable traefik"
  - mv /tmp/kubeconfig /home/spike/.config/kube-config.yaml
  - cat /etc/oem/kube-config.yaml >> /home/spike/.config/kube-config.yaml
  - chown spike:spike /home/spike/.config/kube-config.yaml

#HELM
  - curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash

#NGINX INGRESS
  - helm install --kubeconfig /home/spike/.config/kube-config.yaml --install ingress-nginx ingress-nginx --repo https://kubernetes.github.io/ingress-nginx --namespace ingress-nginx --create-namespace 

#PROMETHEUS
  - kubectl create ns prom
  - helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
  - helm repo update prometheus-community
  - helm install --kubeconfig /home/spike/.config/kube-config.yaml  --namespace prom prom-stack -f /etc/oem/prometheus-stack-values.yaml prometheus-community/kube-prometheus-stack

#READY
  - wall "BUFFY WILL PATROL TONIGHT" 
