#cloud-config
# vim: syntax=yaml
#

# Set your hostname here, the manage_etc_hosts will update the hosts file entries as well
hostname: buffy 
manage_etc_hosts: false 

package_update: false
package_upgrade: false
package_reboot_if_required: true

packages:
  - vim
  - dnsmasq
  - bind9-utils
  - iptables
  - iptables-persistent
  - libarchive-tools
  - nfs-server
  - lighttpd
  - git

users:
  - default

write_files:
- path: /etc/hosts
  permissions: 0644
  owner: 'root:root'
  content: |
    127.0.1.1 buffy buffy
    127.0.0.1 localhost
    ::1 ip6-localhost ip6-loopback
    fe00::0 ip6-localnet
    ff00::0 ip6-mcastprefix
    ff02::1 ip6-allnodes
    ff02::2 ip6-allrouters
    ff02::3 ip6-allhosts

    192.168.1.64 prometheus.buffy
    192.168.1.64 grafana.buffy
    192.168.64.1 buffy
    192.168.64.1 buffy.scooby
    192.168.64.1 buffy.scooby.eastwood
    192.168.64.64 willow
    192.168.64.64 willow.scooby
    192.168.64.64 willow.scooby.eastwood

- path: /etc/dnsmasq.conf
  permissions: 0644
  owner: 'root:root'
  content: |
    listen-address=::1,127.0.0.1,192.168.64.1
    port=53
    domain=scooby.eastwood
    interface=eth1
    bind-interfaces
    dhcp-authoritative
    dhcp-range=interface:eth1,192.168.64.64,192.168.64.191
    dhcp-option=66,192.168.64.1
    dhcp-option=option:router,192.168.64.1
    server=192.168.1.2
    server=1.1.1.1
    expand-hosts
    log-queries
    log-dhcp
    enable-tftp
    tftp-root=/tftpboot/
    pxe-service=0,"Raspberry Pi Boot"
    #dhcp-ignore=tag:!known
    dhcp-host=net:willow,b8:27:eb:86:53:3c,willow,192.168.64.64,24h
    dhcp-option=net:willow,17,192.168.64.1:/var/lib/scooby/agents/willow

- path: /etc/iptables/rules.v4
  permissions: 0644
  owner: 'root:root'
  content: |
    *filter
    :INPUT ACCEPT [5610749:3014554876]
    :FORWARD ACCEPT [0:0]
    :OUTPUT ACCEPT [4835418:1436051403]
    -A FORWARD -i eth0 -o eth1 -m state --state RELATED,ESTABLISHED -j ACCEPT
    -A FORWARD -i eth1 -o eth0 -j ACCEPT
    COMMIT
    *nat
    :PREROUTING ACCEPT [95416:7110036]
    :INPUT ACCEPT [94707:7084699]
    :OUTPUT ACCEPT [96642:5810991]
    :POSTROUTING ACCEPT [96483:5789063]
    -A POSTROUTING -o eth0 -j MASQUERADE
    COMMIT

- path: /etc/exports
  permissions: 0644
  owner: 'root:root'
  content: |
    /var/lib/scooby/agents/willow		willow(rw,sync,no_subtree_check,no_root_squash)

- path: /etc/lighttpd/lighttpd.conf
  permissions: 0644
  owner: 'root:root'
  content: |
    server.bind 		= "192.168.64.1"
    server.port 		= 58087
    server.document-root        = "/var/www/html"
    server.upload-dirs          = ( "/var/cache/lighttpd/uploads" )
    server.errorlog             = "/var/log/lighttpd/error.log"
    server.pid-file             = "/run/lighttpd.pid"
    server.username             = "www-data"
    server.groupname            = "www-data"

- path: /etc/dhcpcd/dhclient.conf
  permissions: 0600
  owner: 'root:root'
  content: |
    option rfc3442-classless-static-routes code 121 = array of unsigned integer 8;
    send host-name = gethostname();
    request subnet-mask, broadcast-address, time-offset, routers,
      domain-name, domain-name-servers, domain-search, host-name,
      dhcp6.name-servers, dhcp6.domain-search, dhcp6.fqdn, dhcp6.sntp-servers,
      netbios-name-servers, netbios-scope, interface-mtu,
      rfc3442-classless-static-routes, ntp-servers;
    #only allow on public interface
    #https://forums.raspberrypi.com/viewtopic.php?t=308463
    denyinterfaces veth*, eth1

- path: /etc/avahi/avahi-daemon.conf
  permissions: 0600
  owner: 'root:root'
  content: |
    [server]
    use-ipv4=yes
    use-ipv6=yes
    #only allow on public interface
    allow-interfaces=eth0
    ratelimit-interval-usec=1000000
    ratelimit-burst=1000

    [wide-area]
    enable-wide-area=yes

    [publish]
    publish-hinfo=no
    publish-workstation=no

- path: /etc/rancher/k3s/config.yaml
  permissions: 0600
  owner: 'root:root'
  content: |
    kube-controller-manager-arg:
    - "bind-address=0.0.0.0"
    kube-proxy-arg:
    - "metrics-bind-address=0.0.0.0"
    kube-scheduler-arg:
    - "bind-address=0.0.0.0"
    # Controller Manager exposes etcd sqllite metrics
    etcd-expose-metrics: true

- path: /var/lib/rancher/k3s/server/manifests/ingress-nginx.yaml
  permissions: 0600
  owner: 'root:root'
  content: |
    apiVersion: v1
    kind: Namespace
    metadata:
      name: ingress-nginx
      labels:
        name: traffic
    ---
    apiVersion: helm.cattle.io/v1
    kind: HelmChart
    metadata:
      name: nginx
      namespace: ingress-nginx
    spec:
      chart: ingress-nginx
      targetNamespace: ingress-nginx
      repo: https://kubernetes.github.io/ingress-nginx
      valuesContent: |-
        image:
          tag: master
        env:
          GF_EXPLORE_ENABLED: true
        adminUser: admin
        sidecar:
          datasources:
            enabled: true

- path: /var/lib/rancher/k3s/server/manifests/prometheus-stack.yaml
  permissions: 0600
  owner: 'root:root'
  content: |
    apiVersion: v1
    kind: Namespace
    metadata:
      name: prom-stack
      labels:
        name: monitoring
    ---
    apiVersion: helm.cattle.io/v1
    kind: HelmChart
    metadata:
      name: prom-stack
      namespace: prom-stack
    spec:
      chart: kube-prometheus-stack
      targetNamespace: prom-stack
      repo: https://prometheus-community.github.io/helm-charts
      valuesContent: |-
        # k3s uses sqllite, so we cannot monitor etcd in the same way
        defaultRules:
          rules:
            etcd: false

        kubeEtcd:
          enabled: false

        # matched to service port 'prom-stack-kube-prometheus-kube-controller-manager' -n kube-system
        kubeControllerManager:
          enabled: true
          endpoints: ['192.168.64.1']
          service:
            enabled: true
            port: 10252
            targetPort: 10252
          serviceMonitor:
            enabled: true
            https: false

        # matched to service port 'prom-stack-kube-prometheus-kube-scheduler' -n kube-system
        kubeScheduler:
          enabled: true
          endpoints: ['192.168.64.1']
          service:
            enabled: true
            port: 10251
            targetPort: 10251
          serviceMonitor:
            enabled: true
            https: false

        # matched to service port 'prom-stack-kube-prometheus-kube-proxy' -n kube-system
        kubeProxy:
          enabled: true
          endpoints: ['192.168.64.1']
          service:
            enabled: true
            port: 10249
            targetPort: 10249


        alertmanager:
          ingress:
            enabled: true
            annotations:
              kubernetes.io/ingress.class: nginx
            hosts: ['alertmanager.buffy']
            paths: ['/']
            tls:
            - secretName: tls-credential
              hosts:
              - alertmanager.buffy

          alertmanagerSpec:
            storage:
              volumeClaimTemplate:
                spec:
                  storageClassName: local-path
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 5Gi
            externalUrl: https://alertmanager.buffy/
            routePrefix: /

          tplConfig: false
          # INSTEAD rely on helm --set-file "alertmanager.templateFiles.email\.default\.html\.tmpl"=pathtofile
          #templateFiles:
          #  email.default.html.tmpl: |-
          #    ... 
          #  email.default.txt.tmpl: |-
          #    ...

          config:
            global:
              resolve_timeout: 5m
              # global smtp settings
              smtp_from: amgr@buffy
              smtp_smarthost: 10.43.235.116:1025
              smtp_require_tls: false

            route:
              group_by: ['alertname']
              group_wait: 2s # not default 30
              group_interval: 30s # not default 5m
              repeat_interval: 4h # not default 12h
              receiver: email_platform
              routes:
              - receiver: 'null'
                matchers:
                  - alertname =~ "InfoInhibitor|Watchdog"
              - receiver: email_platform
                continue: true
            receivers:
            - name: email_platform
              email_configs:
              - to: platform@buffy
                send_resolved: true
                headers:
                  subject: "{{ .Status | toUpper }} {{ .CommonLabels.env }}:{{ .CommonLabels.cluster }} {{ .CommonLabels.alertname }}"
                #html: "{{ range .Alerts }}{{ .Annotations.description }}<br/>{{ end }}"
                # proper syntax for external template ready by alertmanager
                # defining both these values will send email in multipart/alternative
                #html: '{{ template "emaildefaulthtml" . }}'
                #text: '{{ template "emaildefaulttxt" . }}'

            - name: 'null'
            templates:
            - '/etc/alertmanager/config/*.tmpl'

        grafana:
          # username is 'admin'
          adminPassword: grafana
          ingress:
            enabled: true
            annotations:
              kubernetes.io/ingress.class: nginx
            hosts: ['grafana.buffy']
            path: "/"
            tls:
            - secretName: tls-credential
              hosts:
              - grafana.buffy

        prometheus:
          ingress:
            enabled: true
            annotations:
              kubernetes.io/ingress.class: nginx
            hosts: ['prometheus.buffy']
            paths: ['/']
            tls:
            - secretName: tls-credential
              hosts:
              - prometheus.buffy

          prometheusSpec:
            externalUrl: "https://prometheus.buffy/"
            routePrefix: /

            # do not require new PrometheusRule to have all the helm labels in order to match
            ruleSelectorNilUsesHelmValues: false

            # additional scrape job
            additionalScrapeConfigs:
              - job_name: kubernetes-pod-endpoints
                kubernetes_sd_configs:
                - {role: pod}
                relabel_configs:
                - action: keep
                  regex: true
                  source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
                - action: keep
                  regex: true
                  source_labels: [__meta_kubernetes_pod_annotationpresent_prometheus_io_port]
                - action: drop
                  regex: (kube-system|prom)
                  source_labels: [__meta_kubernetes_namespace]
                - action: replace
                  regex: (https?)
                  source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scheme]
                  target_label: __scheme__
                - action: replace
                  regex: (.+)
                  source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]
                  target_label: __metrics_path__
                - action: replace
                  regex: ([^:]+)(?::\d+)?;(\d+)
                  replacement: $1:$2
                  source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]
                  target_label: __address__
                - {action: labelmap, regex: __meta_kubernetes_pod_label_(.+)}
                - action: replace
                  source_labels: [__meta_kubernetes_namespace]
                  target_label: kubernetes_namespace
                - action: replace
                  source_labels: [__meta_kubernetes_service_name]
                  target_label: kubernetes_name
              - job_name: kubernetes-service-endpoints
                kubernetes_sd_configs:
                - {role: service}
                relabel_configs:
                - action: keep
                  regex: true
                  source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]
                - action: drop
                  regex: (kube-system|prom)
                  source_labels: [__meta_kubernetes_namespace]
                - action: keep
                  regex: .*metrics
                  source_labels: [__meta_kubernetes_service_port_name]
                - action: replace
                  regex: (https?)
                  source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]
                  target_label: __scheme__
                - action: replace
                  regex: (.+)
                  source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]
                  target_label: __metrics_path__
                - action: replace
                  regex: ([^:]+)(?::\d+)?;(\d+)
                  replacement: $1:$2
                  source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]
                  target_label: __address__
                - {action: labelmap, regex: __meta_kubernetes_service_label_(.+)}
                - action: replace
                  source_labels: [__meta_kubernetes_namespace]
                  target_label: kubernetes_namespace
                - action: replace
                  source_labels: [__meta_kubernetes_service_name]
                  target_label: kubernetes_name


            # external labels will be common for all alerts and available for templating in AlertManager
            externalLabels: {'cluster': 'k3s', 'env': 'dev', 'jumpbox': 'localhost.local'}

            storageSpec:
              volumeClaimTemplate:
                spec:
                  storageClassName: local-path
                  accessModes: ["ReadWriteOnce"]
                  resources:
                    requests:
                      storage: 5Gi

bootcmd:
  #this must be here for dnsmasq startup
  - mkdir -p /tftpboot
  #install directory
  - mkdir -p /var/lib/scooby
  #this must be here for nfs-kernel-server startup
  - mkdir -p /var/lib/scooby/agents/willow
  #willow install directory
  - mkdir -p /var/lib/scooby/agents/willow/var/lib/scooby

runcmd:
#CREATE CONFIG DIRS
  - mkdir -p /root/.kube

#SSH KEYS
  - ssh-keygen -q -f /home/spike/.ssh/id_ed25519 -N "" -t ed25519
  - cat /home/spike/.ssh/id_ed25519.pub >> /home/spike/.ssh/authorized_keys
  - chown -R spike:spike /home/spike/.ssh/

#SETUP TFTPBOOT
  - cp /boot/bootcode.bin /tftpboot

#WILLOW ROOTFS
  - mkdir -p /mnt/rootfs
  - mount -o loop,offset=272629760 /images/rpi-cloud-init-raspios-bullseye-arm64.img /mnt/rootfs
  - rsync -xa --progress /mnt/rootfs/* /var/lib/scooby/agents/willow
  - cp /agents/willow/fstab /var/lib/scooby/agents/willow/etc
  - umount /mnt/rootfs
  - sleep 5

#WILLOW BOOT
  - mkdir -p /mnt/boot
  - mount -o loop,offset=4194304 /images/rpi-cloud-init-raspios-bullseye-arm64.img /mnt/boot
  - rsync -xa --progress /mnt/boot/* /var/lib/scooby/agents/willow/boot
  - umount /mnt/boot
  - sleep 5
  - cp /agents/willow/user-data /var/lib/scooby/agents/willow/boot
  - cp /agents/willow/meta-data /var/lib/scooby/agents/willow/boot 
  - cp /agents/willow/network-config /var/lib/scooby/agents/willow/boot 
  - cp /agents/willow/cmdline.txt /var/lib/scooby/agents/willow/boot 
  - cp /agents/willow/ssh /var/lib/scooby/agents/willow/boot
  - ln -s /var/lib/scooby/agents/willow/boot /tftpboot/9686533c

#WILLOW CLOUD CONFIG 
  - mkdir -p /var/www/html/cloud-config/willow
  - ln -s /var/lib/scooby/agents/willow/boot/user-data /var/www/html/cloud-config/willow
  - ln -s /var/lib/scooby/agents/willow/boot/meta-data /var/www/html/cloud-config/willow

#WILLOW KEYS
  - mkdir -p /var/lib/scooby/agents/willow/var/lib/scooby/ssh/
  - cp /home/spike/.ssh/id_ed25519 /var/lib/scooby/agents/willow/var/lib/scooby/ssh/spike_ed25519_key
  - cp /home/spike/.ssh/id_ed25519.pub /var/lib/scooby/agents/willow/var/lib/scooby/ssh/spike_ed25519_key.pub

#K3S
  - cd /tmp && curl -sLS https://get.k3sup.dev | sh
  # https://github.com/k3s-io/k3s/issues/535#issuecomment-863188327 
  - k3sup install --ip 192.168.1.64 --user spike --ssh-key "/home/spike/.ssh/id_ed25519" --k3s-extra-args "--flannel-backend=host-gw --disable traefik"
  - cp /tmp/kubeconfig /root/.kube/config

#READY
  - wall "BUFFY WILL PATROL TONIGHT" 
